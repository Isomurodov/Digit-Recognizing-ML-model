{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ecaed732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "756070f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pd = pd.read_csv('/Users/javohir/Downloads/digit-recognizer/train.csv')\n",
    "test_pd = pd.read_csv('/Users/javohir/Downloads/digit-recognizer/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "050685fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_pd.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84a02e3",
   "metadata": {},
   "source": [
    "Splitting the data into train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bcfb6436",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train,  y_test = train_test_split(train_pd.iloc[:, 1:], train_pd['label'], test_size = 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dabbe56",
   "metadata": {},
   "source": [
    "Converting the pandas df to numpy and transposing them to use in the later stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6537b3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "x_train = x_train.T\n",
    "x_test = x_test.T\n",
    "y_train = y_train.T\n",
    "y_test = y_test.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faf6e36",
   "metadata": {},
   "source": [
    "Doing the same with the test (not x_test) dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "34181b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test_pd.to_numpy()\n",
    "\n",
    "test = test.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a1e818",
   "metadata": {},
   "source": [
    "Executing one-hot encoding on the y_train data (becuase we use softmax regression, and in order for it to work properly, we need our Y one-hot encoded):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a190218c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 1., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = np.zeros((y_train.size, y_train.max()+1))\n",
    "temp[np.arange(y_train.size), y_train] = 1\n",
    "y_train = temp.T\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5253cb8",
   "metadata": {},
   "source": [
    "A lazy way of getting an idea and image of how our data may look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c8ad2863",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(100):\n",
    "#    plt.imshow(np.array(train[i, 1:]).reshape(28, 28))\n",
    "  # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43333b74",
   "metadata": {},
   "source": [
    "In the following cells, I will be implementing the helper functions that I will use in my final model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "27f271d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(layer_dims): # initialize parameters W and b\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims) # number of layers in the network\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6c63b418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forw(A, W, b): # linear W * X + b for forward prop\n",
    "    \n",
    "    Z = np.dot(W, A) + b\n",
    "    \n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fe8bf944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z): # ReLU function\n",
    "    \n",
    "    A = Z * (Z > 0)\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6999ba1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c927aa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z): # Softmax function\n",
    "    \n",
    "    A = np.exp(Z) / np.sum(np.exp(Z), axis=0)\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "effb3bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forw(A_prev, W, b, activation):\n",
    "    \n",
    "    Z, linear_cache = linear_forw(A_prev, W, b)\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    elif activation =='softmax':\n",
    "        A, activation_cache = softmax(Z)\n",
    "    \n",
    "    cache = (linear_cache, activation_cache)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8ea03253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forw(X, parameters): # forward propogation with [LINEAR->RELU]*(L-1) -> LINEAR -> SOFTMAX (implements relu L-1 times and performs softmax in the last step)\n",
    "    \n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2 # num of layers in the NN\n",
    "    \n",
    "    # relu * (L - 1)\n",
    "    for l in range(1, L): # loop starts at one because layer 0 is the input\n",
    "        \n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forw(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], 'relu')\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # softmax in the final layer\n",
    "    AL, cache = linear_activation_forw(A, parameters['W' + str(L)], parameters['b' + str(L)], 'softmax')\n",
    "    caches.append(cache)\n",
    "    \n",
    "\n",
    "    #print(AL.shape)\n",
    "    #AL = np.argmax(AL, axis=0)\n",
    "    \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5626e289",
   "metadata": {},
   "source": [
    "Compute the cross-entropy cost $J$, using the following formula: $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))Â \\tag{7}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ea77db58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y): \n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    AL = np.argmax(AL, axis=0)\n",
    "    Y = np.argmax(Y, axis=0)\n",
    "    cost = (sum(AL != Y) / m) * 100\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d088ee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache): # linear backward (LINEAR) for single layer l\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = np.dot(dZ, A_prev.T) / m\n",
    "    db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "219b1c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dZ_relu(dA, cache):\n",
    "    #Implement the backward propagation for a single RELU unit.\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5d12c0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dZ_sigmoid(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "52241240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(AL, Y, dA, cache, activation): # backward prop for LINEAR -> ACTIVATION\n",
    "    \n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == 'relu':\n",
    "        dZ = dZ_relu(dA, activation_cache)\n",
    "    elif activation == 'softmax':\n",
    "        dZ = AL - Y  # ATTENTION!!!!\n",
    "\n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a9909d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    #Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivative of cost with respect to AL\n",
    "    \n",
    "    current_cache = caches[L - 1]\n",
    "    dA_prev_temp, dW_temp, db_temp = linear_activation_backward(AL, Y, dAL, current_cache, activation=\"softmax\")\n",
    "    grads[\"dA\" + str(L-1)] = dA_prev_temp\n",
    "    grads[\"dW\" + str(L)] = dW_temp\n",
    "    grads[\"db\" + str(L)] = db_temp\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(AL, Y, dA_prev_temp, current_cache, activation=\"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8e47c0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(params, grads, learning_rate):\n",
    "    \n",
    "    parameters = params.copy()\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    for l in range(L):\n",
    "        parameters['W' + str(l + 1)] = parameters['W' + str(l + 1)] - learning_rate * grads['dW' + str(l + 1)]\n",
    "        parameters['b' + str(l + 1)] = parameters['b' + str(l + 1)] - learning_rate * grads['db' + str(l + 1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8117def8",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims = [784, 40, 20, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83bc4c2",
   "metadata": {},
   "source": [
    "Finally, the following is the final function that uses the above helper functions and trains the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3198dfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.009, num_iterations = 3000, print_cost=False):\n",
    "    \n",
    "    \"Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SOFTMAX.\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    costs = []\n",
    "    \n",
    "    parameters = init_params(layers_dims)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        #print(i, end=' ')\n",
    "        AL, caches = L_model_forw(X, parameters)\n",
    "        \n",
    "        cost = compute_cost(AL, Y)\n",
    "        \n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        \n",
    "        parameters = update_params(parameters, grads, learning_rate)\n",
    "        \n",
    "        if print_cost and i % 10 == 0 or i == num_iterations - 1:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, cost))\n",
    "        if i % 10 == 0 or i == num_iterations:\n",
    "            costs.append(cost)\n",
    "                \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "31b57ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 92.88253968253967\n",
      "Cost after iteration 10: 83.87301587301587\n",
      "Cost after iteration 20: 80.15555555555555\n",
      "Cost after iteration 30: 53.22539682539682\n",
      "Cost after iteration 40: 78.1015873015873\n",
      "Cost after iteration 50: 30.907936507936505\n",
      "Cost after iteration 60: 38.63174603174603\n",
      "Cost after iteration 70: 42.044444444444444\n",
      "Cost after iteration 80: 24.352380952380955\n",
      "Cost after iteration 90: 18.83809523809524\n",
      "Cost after iteration 100: 14.428571428571429\n",
      "Cost after iteration 110: 13.260317460317461\n",
      "Cost after iteration 120: 17.034920634920635\n",
      "Cost after iteration 130: 11.657142857142858\n",
      "Cost after iteration 140: 10.952380952380953\n",
      "Cost after iteration 150: 10.463492063492064\n",
      "Cost after iteration 160: 10.711111111111112\n",
      "Cost after iteration 170: 10.75873015873016\n",
      "Cost after iteration 180: 9.511111111111111\n",
      "Cost after iteration 190: 9.222222222222221\n",
      "Cost after iteration 200: 8.965079365079365\n",
      "Cost after iteration 210: 8.653968253968253\n",
      "Cost after iteration 220: 8.422222222222222\n",
      "Cost after iteration 230: 8.234920634920636\n",
      "Cost after iteration 240: 8.019047619047619\n",
      "Cost after iteration 250: 8.34920634920635\n",
      "Cost after iteration 260: 8.904761904761905\n",
      "Cost after iteration 270: 7.412698412698412\n",
      "Cost after iteration 280: 7.238095238095238\n",
      "Cost after iteration 290: 7.0476190476190474\n",
      "Cost after iteration 300: 6.8920634920634924\n",
      "Cost after iteration 310: 6.73968253968254\n",
      "Cost after iteration 320: 6.619047619047619\n",
      "Cost after iteration 330: 6.46984126984127\n",
      "Cost after iteration 340: 6.444444444444445\n",
      "Cost after iteration 350: 6.926984126984127\n",
      "Cost after iteration 360: 7.088888888888889\n",
      "Cost after iteration 370: 6.003174603174603\n",
      "Cost after iteration 380: 5.815873015873016\n",
      "Cost after iteration 390: 5.733333333333333\n",
      "Cost after iteration 400: 5.5968253968253965\n",
      "Cost after iteration 410: 5.476190476190476\n",
      "Cost after iteration 420: 5.377777777777778\n",
      "Cost after iteration 430: 5.288888888888889\n",
      "Cost after iteration 440: 5.180952380952381\n",
      "Cost after iteration 450: 5.073015873015873\n",
      "Cost after iteration 460: 5.066666666666666\n",
      "Cost after iteration 470: 5.038095238095238\n",
      "Cost after iteration 480: 5.095238095238095\n",
      "Cost after iteration 490: 5.4031746031746035\n",
      "Cost after iteration 500: 5.2253968253968255\n",
      "Cost after iteration 510: 4.723809523809524\n",
      "Cost after iteration 520: 4.5396825396825395\n",
      "Cost after iteration 530: 4.466666666666667\n",
      "Cost after iteration 540: 4.393650793650793\n",
      "Cost after iteration 550: 4.3238095238095235\n",
      "Cost after iteration 560: 4.238095238095238\n",
      "Cost after iteration 570: 4.190476190476191\n",
      "Cost after iteration 580: 4.12063492063492\n",
      "Cost after iteration 590: 4.104761904761904\n",
      "Cost after iteration 600: 4.104761904761904\n",
      "Cost after iteration 610: 4.152380952380953\n",
      "Cost after iteration 620: 4.428571428571428\n",
      "Cost after iteration 630: 4.923809523809524\n",
      "Cost after iteration 640: 3.8857142857142852\n",
      "Cost after iteration 650: 3.8031746031746034\n",
      "Cost after iteration 660: 3.7555555555555555\n",
      "Cost after iteration 670: 3.7206349206349207\n",
      "Cost after iteration 680: 3.6920634920634923\n",
      "Cost after iteration 690: 3.6222222222222227\n",
      "Cost after iteration 700: 3.638095238095238\n",
      "Cost after iteration 710: 3.7682539682539686\n",
      "Cost after iteration 720: 9.898412698412699\n",
      "Cost after iteration 730: 7.666666666666666\n",
      "Cost after iteration 740: 5.907936507936508\n",
      "Cost after iteration 750: 5.298412698412698\n",
      "Cost after iteration 760: 4.946031746031746\n",
      "Cost after iteration 770: 4.692063492063491\n",
      "Cost after iteration 780: 4.46984126984127\n",
      "Cost after iteration 790: 4.26984126984127\n",
      "Cost after iteration 800: 4.111111111111112\n",
      "Cost after iteration 810: 4.012698412698413\n",
      "Cost after iteration 820: 3.9301587301587304\n",
      "Cost after iteration 830: 3.780952380952381\n",
      "Cost after iteration 840: 3.711111111111111\n",
      "Cost after iteration 850: 3.6126984126984127\n",
      "Cost after iteration 860: 3.5333333333333337\n",
      "Cost after iteration 870: 3.4666666666666663\n",
      "Cost after iteration 880: 3.3746031746031746\n",
      "Cost after iteration 890: 3.3079365079365077\n",
      "Cost after iteration 900: 3.2444444444444445\n",
      "Cost after iteration 910: 3.212698412698413\n",
      "Cost after iteration 920: 3.1904761904761907\n",
      "Cost after iteration 930: 3.1428571428571432\n",
      "Cost after iteration 940: 3.0825396825396827\n",
      "Cost after iteration 950: 3.0476190476190474\n",
      "Cost after iteration 960: 3.0253968253968253\n",
      "Cost after iteration 970: 2.9587301587301584\n",
      "Cost after iteration 980: 2.892063492063492\n",
      "Cost after iteration 990: 2.86984126984127\n",
      "Cost after iteration 1000: 2.8285714285714287\n",
      "Cost after iteration 1010: 2.8000000000000003\n",
      "Cost after iteration 1020: 2.784126984126984\n",
      "Cost after iteration 1030: 2.768253968253968\n",
      "Cost after iteration 1040: 2.7587301587301587\n",
      "Cost after iteration 1050: 2.746031746031746\n",
      "Cost after iteration 1060: 2.679365079365079\n",
      "Cost after iteration 1070: 2.6095238095238096\n",
      "Cost after iteration 1080: 2.5587301587301585\n",
      "Cost after iteration 1090: 2.526984126984127\n",
      "Cost after iteration 1100: 2.4793650793650794\n",
      "Cost after iteration 1110: 2.447619047619048\n",
      "Cost after iteration 1120: 2.4063492063492067\n",
      "Cost after iteration 1130: 2.3777777777777778\n",
      "Cost after iteration 1140: 2.3555555555555556\n",
      "Cost after iteration 1150: 2.3333333333333335\n",
      "Cost after iteration 1160: 2.314285714285714\n",
      "Cost after iteration 1170: 2.2634920634920634\n",
      "Cost after iteration 1180: 2.2444444444444445\n",
      "Cost after iteration 1190: 2.193650793650794\n",
      "Cost after iteration 1200: 2.1714285714285713\n",
      "Cost after iteration 1210: 2.1523809523809523\n",
      "Cost after iteration 1220: 2.142857142857143\n",
      "Cost after iteration 1230: 2.123809523809524\n"
     ]
    }
   ],
   "source": [
    "parameters, cost = L_layer_model(x_train, y_train, layers_dims, num_iterations = 1231, print_cost = True)  #alpha = 0.00875, layer_dims = [784, 40, 20, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cfe16b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcb8c595fd0>]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "60eda373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.98194356e-01, 9.99996739e-01, 1.45091443e-03, ...,\n",
       "        1.63049931e-07, 3.89747739e-11, 7.95059101e-04],\n",
       "       [6.17267731e-08, 1.22125472e-12, 4.70104036e-07, ...,\n",
       "        9.91000162e-01, 9.99333152e-01, 1.43601593e-04],\n",
       "       [8.88525178e-04, 9.79150878e-07, 6.35289111e-01, ...,\n",
       "        1.28078099e-03, 2.79951073e-04, 1.15121513e-03],\n",
       "       ...,\n",
       "       [9.84700232e-05, 2.25461599e-08, 7.18865972e-04, ...,\n",
       "        6.18610035e-03, 2.13143230e-04, 9.21690139e-06],\n",
       "       [7.62257613e-06, 2.05402928e-09, 1.34667134e-04, ...,\n",
       "        5.61512372e-04, 1.59367403e-04, 4.32344688e-05],\n",
       "       [2.88133505e-04, 1.84386369e-07, 5.78300442e-06, ...,\n",
       "        1.27778768e-04, 1.13818830e-06, 1.13688987e-06]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output, _ = L_model_forw(x_test, parameters)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec17da5",
   "metadata": {},
   "source": [
    "Converting the probabilities to the actual classifier values (i.e: 0, 1, .. , 9):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6ed3e8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = np.argmax(output, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75c9002",
   "metadata": {},
   "source": [
    "Measuring the accuracy of the model by comparing the output to the ground-truth data (y_test):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0440f804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9585714285714285\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: ', sum(output == y_test) / len(output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
